{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EVA4 - S4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ckgpeace/EVA4/blob/master/S4/EVA4_S4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m2JWFliFfKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_Cx9q2QFgM7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model 6:\n",
        "# Things Tried: 1. Maxpooling 2. Dropout 4. Batch Normalization\n",
        "# Accuracy: 99.4% in 18th epoch\n",
        "# Params = 11,178\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(Net, self).__init__()\n",
        "      \n",
        "      self.conv1 = nn.Conv2d(1, 8, 3)     #input-28 | Output-26 | RF-3\n",
        "      self.bn1 = nn.BatchNorm2d(8)\n",
        "      self.do1 = nn.Dropout(0.1)\n",
        "\n",
        "      self.conv2 = nn.Conv2d(8, 8, 3)     #input-26 | Output-24 | RF-5\n",
        "      self.bn2 = nn.BatchNorm2d(8)\n",
        "      self.do2 = nn.Dropout(0.1)\n",
        "\n",
        "      self.conv3 = nn.Conv2d(8, 8, 3)     #input-24 | Output-22 | RF-7\n",
        "      self.bn3 = nn.BatchNorm2d(8)\n",
        "      self.do3 = nn.Dropout(0.1)\n",
        "\n",
        "      self.pool1 = nn.MaxPool2d(2, 2)     #input-22 | Output-11 | RF-14\n",
        "      \n",
        "      self.conv4 = nn.Conv2d(8, 16, 3)    #input-11 | Output-09 | RF-16\n",
        "      self.bn4 = nn.BatchNorm2d(16)\n",
        "      self.do4 = nn.Dropout(0.1)\n",
        "\n",
        "      self.conv5 = nn.Conv2d(16, 16, 3)   #input-09 | Output-07 | RF-18\n",
        "      self.bn5 = nn.BatchNorm2d(16)\n",
        "      self.do5 = nn.Dropout(0.1)\n",
        "\n",
        "      self.conv6 = nn.Conv2d(16, 16, 3)   #input-07 | Output-05 | RF-20\n",
        "      self.bn6 = nn.BatchNorm2d(16)\n",
        "      self.do6 = nn.Dropout(0.1)\n",
        "\n",
        "      self.conv7 = nn.Conv2d(16, 10, 5)   #input-04 | Output-01 |\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = F.relu(self.conv1(x))\n",
        "      x = self.bn1(x)\n",
        "      x = self.do1(x)\n",
        "\n",
        "      x = F.relu(self.conv2(x))\n",
        "      x = self.bn2(x)\n",
        "      x = self.do2(x)\n",
        "\n",
        "      x = F.relu(self.conv3(x))\n",
        "      x = self.bn3(x)\n",
        "      x = self.do3(x)\n",
        "\n",
        "      x = self.pool1(x)\n",
        "\n",
        "      x = F.relu(self.conv4(x))\n",
        "      x = self.bn4(x)\n",
        "      x = self.do4(x)\n",
        "\n",
        "      x = F.relu(self.conv5(x))\n",
        "      x = self.bn5(x)\n",
        "      x = self.do5(x)\n",
        "\n",
        "      x = F.relu(self.conv6(x)) # No Batch Normalization and drop out before final layer\n",
        "\n",
        "      x = self.conv7(x) # No Relu function before softmax layer\n",
        "\n",
        "      x = x.view(-1, 10)\n",
        "      \n",
        "      return F.log_softmax(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xdydjYTZFyi3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "outputId": "3995b2b5-57eb-4019-8cf9-193e2466d1dd"
      },
      "source": [
        "# !pip install torchsummary\n",
        "from torchsummary import summary\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "model = Net().to(device)\n",
        "summary(model, input_size=(1, 28, 28))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [-1, 8, 26, 26]              80\n",
            "       BatchNorm2d-2            [-1, 8, 26, 26]              16\n",
            "           Dropout-3            [-1, 8, 26, 26]               0\n",
            "            Conv2d-4            [-1, 8, 24, 24]             584\n",
            "       BatchNorm2d-5            [-1, 8, 24, 24]              16\n",
            "           Dropout-6            [-1, 8, 24, 24]               0\n",
            "            Conv2d-7            [-1, 8, 22, 22]             584\n",
            "       BatchNorm2d-8            [-1, 8, 22, 22]              16\n",
            "           Dropout-9            [-1, 8, 22, 22]               0\n",
            "        MaxPool2d-10            [-1, 8, 11, 11]               0\n",
            "           Conv2d-11             [-1, 16, 9, 9]           1,168\n",
            "      BatchNorm2d-12             [-1, 16, 9, 9]              32\n",
            "          Dropout-13             [-1, 16, 9, 9]               0\n",
            "           Conv2d-14             [-1, 16, 7, 7]           2,320\n",
            "      BatchNorm2d-15             [-1, 16, 7, 7]              32\n",
            "          Dropout-16             [-1, 16, 7, 7]               0\n",
            "           Conv2d-17             [-1, 16, 5, 5]           2,320\n",
            "           Conv2d-18             [-1, 10, 1, 1]           4,010\n",
            "================================================================\n",
            "Total params: 11,178\n",
            "Trainable params: 11,178\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.38\n",
            "Params size (MB): 0.04\n",
            "Estimated Total Size (MB): 0.42\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:64: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqTWLaM5GHgH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.manual_seed(1)\n",
        "batch_size = 128\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                    transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fDefDhaFlwH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    pbar = tqdm(train_loader)\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        pbar.set_description(desc= f'loss={loss.item()} batch_id={batch_idx}')\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMWbLWO6FuHb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "03b284a8-44ef-4935-faba-b1fb30b7db38"
      },
      "source": [
        "model = Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "for epoch in range(20):\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/469 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:64: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "loss=0.016010314226150513 batch_id=468: 100%|██████████| 469/469 [00:11<00:00, 40.22it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0691, Accuracy: 9777/10000 (98%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.048252370208501816 batch_id=468: 100%|██████████| 469/469 [00:11<00:00, 40.46it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0490, Accuracy: 9832/10000 (98%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.05628104880452156 batch_id=468: 100%|██████████| 469/469 [00:11<00:00, 40.78it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0379, Accuracy: 9875/10000 (99%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.04582947865128517 batch_id=468: 100%|██████████| 469/469 [00:11<00:00, 40.97it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0343, Accuracy: 9874/10000 (99%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.015893479809165 batch_id=468: 100%|██████████| 469/469 [00:11<00:00, 40.14it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0268, Accuracy: 9918/10000 (99%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.11675681918859482 batch_id=468: 100%|██████████| 469/469 [00:11<00:00, 40.79it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0280, Accuracy: 9916/10000 (99%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.01656544767320156 batch_id=468: 100%|██████████| 469/469 [00:11<00:00, 40.54it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0270, Accuracy: 9922/10000 (99%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.034620266407728195 batch_id=468: 100%|██████████| 469/469 [00:11<00:00, 41.24it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0239, Accuracy: 9926/10000 (99%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.022985652089118958 batch_id=468: 100%|██████████| 469/469 [00:11<00:00, 39.97it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0250, Accuracy: 9923/10000 (99%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.11253893375396729 batch_id=468: 100%|██████████| 469/469 [00:11<00:00, 41.23it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0226, Accuracy: 9925/10000 (99%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.06869157403707504 batch_id=468: 100%|██████████| 469/469 [00:11<00:00, 40.71it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0220, Accuracy: 9929/10000 (99%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.006589561700820923 batch_id=468: 100%|██████████| 469/469 [00:11<00:00, 44.25it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0247, Accuracy: 9927/10000 (99%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.029804876074194908 batch_id=468: 100%|██████████| 469/469 [00:11<00:00, 40.18it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0249, Accuracy: 9925/10000 (99%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.010983586311340332 batch_id=468: 100%|██████████| 469/469 [00:11<00:00, 39.71it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0292, Accuracy: 9918/10000 (99%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.016453862190246582 batch_id=468: 100%|██████████| 469/469 [00:11<00:00, 40.76it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0243, Accuracy: 9924/10000 (99%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.016031155362725258 batch_id=468: 100%|██████████| 469/469 [00:11<00:00, 40.09it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0278, Accuracy: 9919/10000 (99%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.06903522461652756 batch_id=468: 100%|██████████| 469/469 [00:11<00:00, 39.44it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0237, Accuracy: 9921/10000 (99%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.0032838284969329834 batch_id=468: 100%|██████████| 469/469 [00:11<00:00, 42.63it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0225, Accuracy: 9941/10000 (99%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.016547391191124916 batch_id=468: 100%|██████████| 469/469 [00:11<00:00, 39.60it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0273, Accuracy: 9921/10000 (99%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.02401886321604252 batch_id=468: 100%|██████████| 469/469 [00:11<00:00, 42.72it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0254, Accuracy: 9926/10000 (99%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6tVtx90CKq0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgSbjwCB1gSp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        },
        "outputId": "117fd8d8-156f-4667-972d-50da42ffb404"
      },
      "source": [
        "# plot training history\n",
        "# Need to fix this\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "e = epoch\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.subplot(2,1,1)\n",
        "ax = plt.gca()\n",
        "ax.set_xlim([0, e+2])\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(range(1, e + 2), train_losses[:e+1], 'r', label='Training Loss')\n",
        "plt.plot(range(1, e + 2), val_losses[:e+1], 'b', label='Validation Loss')\n",
        "ax.grid(linestyle='-.')\n",
        "plt.legend()\n",
        "plt.subplot(2,1,2)\n",
        "ax = plt.gca()\n",
        "ax.set_xlim([0, e+2])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.plot(range(1, 100), train_accu[:e+1], 'r', label='Training Accuracy')\n",
        "plt.plot(range(1, e + 2), val_accu[:e+1], 'b', label='Validation Accuracy')\n",
        "ax.grid(linestyle='-.')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-731d05c0775f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xlim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validation Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinestyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'-.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_losses' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtAAAAFLCAYAAAD/HruBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngAH/D7zRHeovixmJAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 864x864 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "So5uk4EkHW6R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}